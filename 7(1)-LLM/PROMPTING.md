# 📊 GSM8K Prompting 실습 보고서

본 보고서는 **Llama-3.1-8b-instant** 모델을 활용하여 수학적 추론 능력을 측정하는 **GSM8K 데이터셋**에 대해 프롬프팅 기법별 성능을 분석한 결과이다.

---

## 1. 실험 결과 요약 (Accuracy)

총 50개의 샘플을 대상으로 테스트를 진행하였으며, 각 기법 및 Shot 수에 따른 정답률은 다음과 같다.

| 기법 (Prompting Type) | 0-shot    | 3-shot    | 5-shot    |
| --------------------- | --------- | --------- | --------- |
| **Direct Prompting**  | 80.0%     | 80.0%     | 76.0%     |
| **CoT Prompting**     | 74.0%     | 84.0%     | 80.0%     |
| **My Prompting**      | **80.0%** | **88.0%** | **82.0%** |

---

## 2. CoT vs Direct: 성능 차이 분석

**Chain-of-Thought (CoT)** 프롬프팅이 단순 정답만을 요구하는 **Direct Prompting**보다 높은 성능을 보이는 이유는 다음과 같다.

- **추론의 단계화:** 수학 문제는 복합적인 연산 과정을 거침. CoT는 문제를 작은 단위로 쪼개어 해결하게 함으로써 논리적 오류를 방지함.
- **작업 기억 활용:** 풀이 과정을 직접 생성하는 과정이 모델에게 메모장 역할을 하여, 앞선 계산 결과를 다음 단계에서 정확히 참조할 수 있게 함.

---

## 3. My Prompting: 전략 및 강점

이번 실험에서 사용한 **My Prompting** 기법은 기존 CoT 구조에 **'자기 검증(Verification)'** 단계를 명시적으로 추가한 방식

### 주요 전략: [Analyze - Solve - Verify]

- **Analyze:** 문제에 주어진 조건과 데이터를 먼저 분석하도록 유도
- **Solve:** 분석된 내용을 바탕으로 단계별 풀이 과정을 서술
- **Verify:** "계산을 다시 한번 확인하라"는 지시어를 통해 모델 스스로 산술 오류를 수정할 기회를 부여

### CoT 대비 우수성

- **실수 방지:** 일반 CoT와 달리 최종 답변 직전 검증 단계를 거치며 사소한 계산 실수를 줄임.
- **구조적 안정성:** 명확한 출력 섹션을 구분하여 모델의 사고 흐름을 정돈하고, 정답 추출 로직의 정확도를 높임.

---

## 4. 결론

실험 결과, LLM의 수학적 추론 성능은 단순히 예시를 많이 보여주는 것보다 **모델이 사고하는 방식 자체를 구조화하는 프롬프트 설계**에 더 큰 영향을 받음을 확인함.
